{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24644ea",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: MNIST Deep Dive\n",
    "\n",
    "**Autor:** Samuel Leonardo Albarracín Vergara  \n",
    "**Curso:** AREP  \n",
    "**Fecha:** 03/02/2026\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "Este notebook implementa y analiza una red neuronal convolucional (CNN) desde cero usando MNIST como dataset.\n",
    "Se comparan arquitecturas con y sin convoluciones, se realizan experimentos controlados, y se interpretan las decisiones arquitectónicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f4620",
   "metadata": {},
   "source": [
    "# Parte 1: Exploración de Datos (EDA)\n",
    "\n",
    "## 1.1 Carga del Dataset\n",
    "Utilizamos MNIST desde TensorFlow Keras, que contiene:\n",
    "- **60,000 imágenes de entrenamiento**\n",
    "- **10,000 imágenes de prueba**\n",
    "- Dígitos manuscritos (0-9)\n",
    "- Dimensiones: 28×28 píxeles en escala de grises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc974a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Cargar datos\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(f\"Tamaño de x_train: {x_train.shape}\")\n",
    "print(f\"Tamaño de y_train: {y_train.shape}\")\n",
    "print(f\"Tamaño de x_test: {x_test.shape}\")\n",
    "print(f\"Tamaño de y_test: {y_test.shape}\")\n",
    "print(f\"\\nRango de píxeles: [{x_train.min()}, {x_train.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca139cc",
   "metadata": {},
   "source": [
    "## 1.2 Análisis de Distribución de Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1619635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Contar ejemplos por clase\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "# Crear tabla\n",
    "dist_df = pd.DataFrame({\n",
    "    'Dígito': unique_train,\n",
    "    'Entrenamiento': counts_train,\n",
    "    'Prueba': counts_test\n",
    "})\n",
    "\n",
    "print(\"Distribución de clases:\")\n",
    "print(dist_df)\n",
    "print(f\"\\nTotal entrenamiento: {counts_train.sum()}\")\n",
    "print(f\"Total prueba: {counts_test.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd00fd",
   "metadata": {},
   "source": [
    "**Observación:** Las clases están distribuidas de manera relativamente balanceada (aproximadamente 6000 muestras de entrenamiento por dígito)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc152f1",
   "metadata": {},
   "source": [
    "## 1.3 Visualización de Ejemplos\n",
    "Mostramos un ejemplo de cada dígito para entender la variabilidad del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d316546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ejemplo por clase\n",
    "plt.figure(figsize=(12, 2))\n",
    "for digit in range(10):\n",
    "    idx = np.where(y_train == digit)[0][0]\n",
    "    plt.subplot(1, 10, digit + 1)\n",
    "    plt.imshow(x_train[idx], cmap='gray')\n",
    "    plt.title(f\"{digit}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Un ejemplo de cada dígito', y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c429544",
   "metadata": {},
   "source": [
    "## 1.4 Análisis de Píxeles y Preprocesamiento\n",
    "\n",
    "Analizamos características estadísticas de los píxeles antes del preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d66886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas de píxeles\n",
    "print(\"Estadísticas de píxeles (valor original 0-255):\")\n",
    "print(f\"Min: {x_train.min()}, Max: {x_train.max()}\")\n",
    "print(f\"Media: {x_train.mean():.2f}\")\n",
    "print(f\"Desv. Est.: {x_train.std():.2f}\")\n",
    "print(f\"Mediana: {np.median(x_train):.2f}\")\n",
    "\n",
    "# Histograma de valores\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(x_train.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Intensidad de píxel')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de intensidades de píxeles (Entrenamiento)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c7e23",
   "metadata": {},
   "source": [
    "## 1.5 Normalización\n",
    "\n",
    "Las redes neuronales entrenan mejor con valores normalizados en el rango [0, 1].\n",
    "Dividimos por 255 para convertir píxeles del rango [0, 255] al rango [0, 1].\n",
    "\n",
    "$$x_{normalizado} = \\frac{x_{original}}{255}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090204ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar\n",
    "x_train_norm = x_train / 255.0\n",
    "x_test_norm = x_test / 255.0\n",
    "\n",
    "print(\"Después de normalización:\")\n",
    "print(f\"Min: {x_train_norm.min()}, Max: {x_train_norm.max()}\")\n",
    "print(f\"Media: {x_train_norm.mean():.4f}\")\n",
    "print(f\"Desv. Est.: {x_train_norm.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae458d",
   "metadata": {},
   "source": [
    "## 1.6 Resumen EDA\n",
    "\n",
    "| Característica | Valor |\n",
    "|---|---|\n",
    "| Imágenes de entrenamiento | 60,000 |\n",
    "| Imágenes de prueba | 10,000 |\n",
    "| Dimensiones de imagen | 28 × 28 píxeles |\n",
    "| Canales | 1 (escala de grises) |\n",
    "| Número de clases | 10 (dígitos 0-9) |\n",
    "| Distribuición de clases | Balanceada (~6000 por clase) |\n",
    "| Rango de píxeles original | [0, 255] |\n",
    "| Rango normalizado | [0, 1] |\n",
    "\n",
    "**Conclusión:** MNIST es un dataset ideal para CNN porque:\n",
    "- La información espacial es crucial (píxeles cercanos están correlacionados)\n",
    "- Las convoluciones pueden detectar características locales (bordes, esquinas)\n",
    "- El dataset es pequeño (~1.4 GB en memoria) y se procesa rápidamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hz6jon9sjwm",
   "source": "# Parte 2: Modelo Baseline (Sin Convoluciones)\n\nAntes de construir una CNN, establecemos un punto de referencia con una red **fully connected**.\nEsto nos permite cuantificar cuánto mejora (o no) la CNN frente a un modelo que ignora\nla estructura espacial de la imagen.\n\n## Arquitectura del Baseline\n\n```\nInput (28x28) → Flatten (784) → Dense(128, ReLU) → Dense(10, Softmax)\n```\n\n- **Flatten**: convierte la imagen 2D en un vector 1D de 784 valores. Se pierde toda noción\n  de qué píxeles son vecinos.\n- **Dense(128, ReLU)**: capa oculta con 128 neuronas. Cada neurona recibe los 784 píxeles\n  como entrada independiente (784×128 = 100,352 pesos + 128 biases).\n- **Dense(10, Softmax)**: capa de salida que produce una distribución de probabilidad\n  sobre las 10 clases.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "721t6qjja6",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.utils import to_categorical\n\n# One-hot encoding de las etiquetas\ny_train_oh = to_categorical(y_train, 10)\ny_test_oh = to_categorical(y_test, 10)\n\n# Construir el modelo baseline\nbaseline_model = Sequential([\n    Flatten(input_shape=(28, 28)),\n    Dense(128, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\nbaseline_model.summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "n8owpz6id1s",
   "source": "## 2.1 Compilación y Entrenamiento\n\nUsamos:\n- **Optimizador Adam**: adapta la tasa de aprendizaje por parámetro, converge rápido en la mayoría de casos.\n- **Loss: categorical crossentropy**: función de pérdida estándar para clasificación multiclase con one-hot encoding.\n- **validation_split=0.1**: reservamos el 10% del entrenamiento para validación. Esto nos permite detectar overfitting (si la accuracy de entrenamiento sube pero la de validación se estanca o baja).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p1wu4dh39ej",
   "source": "baseline_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nbaseline_history = baseline_model.fit(\n    x_train_norm, y_train_oh,\n    epochs=10,\n    batch_size=32,\n    validation_split=0.1,\n    verbose=1\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gd6z4pyn0pf",
   "source": "## 2.2 Curvas de Entrenamiento\n\nGraficamos la evolución del loss y la accuracy a lo largo de los epochs, tanto para\nentrenamiento como para validación. Esto permite observar si el modelo está aprendiendo\ncorrectamente o si presenta overfitting.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "14tv4pqwwz9",
   "source": "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Loss\nax1.plot(baseline_history.history['loss'], label='Train')\nax1.plot(baseline_history.history['val_loss'], label='Validation')\nax1.set_title('Baseline — Loss por epoch')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(alpha=0.3)\n\n# Accuracy\nax2.plot(baseline_history.history['accuracy'], label='Train')\nax2.plot(baseline_history.history['val_accuracy'], label='Validation')\nax2.set_title('Baseline — Accuracy por epoch')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy')\nax2.legend()\nax2.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "svosk6ujbri",
   "source": "## 2.3 Evaluación en Test",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "c83cdzbjtp",
   "source": "baseline_loss, baseline_acc = baseline_model.evaluate(x_test_norm, y_test_oh, verbose=0)\n\nprint(f\"Baseline — Test Loss:     {baseline_loss:.4f}\")\nprint(f\"Baseline — Test Accuracy: {baseline_acc:.4f}\")\nprint(f\"\\nTotal de parámetros: {baseline_model.count_params():,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "hkd0mny1mq9",
   "source": "## 2.4 Limitaciones del Modelo Baseline\n\nEl modelo fully connected tiene limitaciones fundamentales para datos de imagen:\n\n1. **Ignora la estructura espacial:** `Flatten` destruye la relación 2D entre píxeles.\n   El modelo no sabe que el píxel (5,5) es vecino del píxel (5,6). Trata una imagen\n   igual que cualquier vector de 784 números sin orden.\n\n2. **Alto número de parámetros:** Solo la primera capa Dense tiene 784×128 = 100,352 pesos.\n   Cada neurona se conecta a *todos* los píxeles, incluso a los que no aportan información\n   (los bordes negros, por ejemplo). Esto es ineficiente.\n\n3. **Sin invariancia a traslación:** Si un dígito \"3\" aparece ligeramente desplazado,\n   el modelo lo ve como un patrón completamente diferente porque los mismos píxeles\n   activan neuronas distintas.\n\n4. **No comparte características:** Si el modelo aprende a detectar un borde vertical\n   en la esquina superior izquierda, no puede reutilizar ese conocimiento para detectar\n   el mismo borde en otra posición de la imagen.\n\nEstas limitaciones son exactamente las que las **capas convolucionales** resuelven mediante\nweight sharing y receptive fields locales.",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}